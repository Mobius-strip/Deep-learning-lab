# -*- coding: utf-8 -*-
"""Minor 2 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IIAtrjTsLH3L0ClN4_IeuAwFtBMXOLVj
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.nn.init as init
# Define device for training
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define CNN model
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=3, padding=1)
        self.conv5 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, padding=1)
        self.conv6 = nn.Conv2d(in_channels=96, out_channels=192, kernel_size=5, padding=1)
        self.conv7 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=5, padding=1)
        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(in_features=384* 14 * 14, out_features=256)
        self.fc2 = nn.Linear(in_features=256, out_features=10)
        init.xavier_uniform_(self.conv1.weight)
        init.xavier_uniform_(self.conv2.weight)
        init.xavier_uniform_(self.conv3.weight)
        init.xavier_uniform_(self.conv4.weight)
        init.xavier_uniform_(self.conv5.weight)
        init.xavier_uniform_(self.conv6.weight)
        init.xavier_uniform_(self.conv7.weight)

    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = nn.functional.relu(self.conv3(x))
        x = nn.functional.relu(self.conv4(x))
        x = nn.functional.relu(self.conv5(x))
        x = nn.functional.relu(self.conv6(x))
        x = self.pool(nn.functional.relu(self.conv7(x)))
        x = x.view(-1, 384* 14 * 14)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Load CIFAR-10 dataset and apply data augmentation
transform_train = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)

test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)
cnn = CNN().to(device)
# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(cnn.parameters(), lr=0.001)

# Commented out IPython magic to ensure Python compatibility.
train_losses=[]
test_losses = []
train_acc = []
val_acc = []
num_epochs=10
for epoch in range(10):
    r=0.0
    correct_train = 0
    total_train = 0
    cnn = cnn.train()
    for batch_idx, (features, targets) in enumerate(train_loader):
        
        features = features.to(device)
        targets = targets.to(device)
        
        ### FORWARD AND BACK PROP
        logits =cnn(features)
        cost = criterion(logits, targets)
        optimizer.zero_grad()
        
        cost.backward()
        optimizer.step()
        r=r+cost.item()
        r /= len(train_loader)
        
        ### UPDATE MODEL PARAMETERS
        
    
        _, predicted = torch.max(logits.data, 1)
        total_train += targets.size(0)
        correct_train += (predicted == targets).sum().item()
        ### LOGGING
        if not batch_idx % 250:
            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' 
#                   %(epoch+1, num_epochs, batch_idx, 
                    len(train_loader), cost))
            
    train_losses.append(r) 
    train_acc.append(100 * correct_train / total_train)
    #val_acc.append(test(model, test_loader))
    test_loss = 0   
    correct = 0
    total = 0 
    with torch.no_grad():
        for data, target in test_loader:
            output = cnn(data.to(device))
            loss = torch.nn.functional.cross_entropy(output, target.to(device))
            test_loss += loss.item()
            _, predicted = torch.max(logits.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
        
    val_acc.append(100 * correct/ total)
    test_loss /= len(test_loader)
    test_losses.append(test_loss)  

    cnn = cnn.eval() # eval mode to prevent upd. batchnorm params during inference
    
    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (
              epoch+1, num_epochs,100 * correct_train / total_train))

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        inputs, labels = data
        inputs=inputs.to(device)
        labels=labels.to(device)
        outputs = cnn(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
print("Accuracy: {:.2f}%".format(100 * correct / total))                                                                                                                                                              +



import matplotlib.pyplot as plt
plt.plot(train_losses, label='training loss')
plt.plot(test_losses, label='testing loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
plt.plot(train_acc, label='training acc')
plt.plot(val_acc, label='val acc')
plt.legend()
plt.show()

"""# Student """

class student_CNN(nn.Module):
    def __init__(self):
        super(student_CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=36, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(in_channels=36, out_channels=64, kernel_size=5, padding=1)
        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(in_features=64* 15* 15, out_features=256)
        self.fc2 = nn.Linear(in_features=256, out_features=10)
        init.xavier_uniform_(self.conv1.weight)
        init.xavier_uniform_(self.conv2.weight)
        init.xavier_uniform_(self.conv3.weight)
        init.xavier_uniform_(self.fc1.weight)
        init.xavier_uniform_(self.fc2.weight)

    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
    
        x = self.pool(nn.functional.relu(self.conv3(x)))
        x = x.view(-1, 64* 15* 15)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x



teacher_net = cnn
student_net =student_CNN().to(device)



learning_rate = 0.001
num_epochs =10

# Define the optimizer and loss function
optimizer = torch.optim.Adam(student_net.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

# Set the temperature and alpha parameters for soft target generation
temperature = 4
alpha =0.5

train_losses=[]
test_losses = []
train_acc = []
val_acc = []

# Train the student network
for epoch in range(num_epochs):
    running_loss = 0.0
    
    # Set the networks to train mode
    student_net.train()
    teacher_net.eval()
    correct_train = 0
    total_train = 0
    
    # Iterate over the training data
    for inputs, labels in train_loader:

        inputs, labels = inputs.to(device), labels.to(device)
        
        
        # Generate the soft and hard targets from the teacher network
        with torch.no_grad():
            soft_target = torch.nn.functional.softmax(teacher_net(inputs) / temperature, dim=1)
            hard_target = torch.zeros_like(soft_target)
            hard_target.scatter_(1, labels.unsqueeze(1), 1)
            target = alpha * soft_target + (1 - alpha) * hard_target
        
        # Forward pass through the student network
        outputs = student_net(inputs)
        loss = criterion(outputs, target)
        
        # Backward pass and optimization step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()

        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()
        
            
    train_losses.append(running_loss / len(train_loader)) 
    train_acc.append(100 * correct_train / total_train)
    #val_acc.append(test(model, test_loader))
    test_loss = 0   
    correct = 0
    total = 0 
    with torch.no_grad():
        for data, target in test_loader:
            output = student_net(data.to(device))
            loss = torch.nn.functional.cross_entropy(output, target.to(device))
            test_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
    val_acc.append(100 * correct/ total)
    test_loss /= len(test_loader)
    test_losses.append(test_loss) 

  
    print(f"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss/len(train_loader) :.4f}")

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        inputs, labels = data
        inputs=inputs.to(device)
        labels=labels.to(device)
        outputs = cnn(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
print("Accuracy: {:.2f}%".format(100 * correct / total))



import matplotlib.pyplot as plt
plt.plot(train_losses, label='training loss')
plt.plot(test_losses, label='testing loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
plt.plot(train_acc, label='training acc')
plt.plot(val_acc, label='val acc')
plt.legend()
plt.show()

student_net =student_CNN().to(device)

# Commented out IPython magic to ensure Python compatibility.
train_losses=[]
test_losses = []
train_acc = []
val_acc = []
num_epochs=10
for epoch in range(10):
    r=0.0
    correct_train = 0
    total_train = 0
    student_net = student_net.train()
    for batch_idx, (features, targets) in enumerate(train_loader):
        
        features = features.to(device)
        targets = targets.to(device)
        
        ### FORWARD AND BACK PROP
        logits =student_net(features)
        cost = criterion(logits, targets)
        optimizer.zero_grad()
        
        cost.backward()
        optimizer.step()
        r=r+cost.item()
        r /= len(train_loader)
        
        ### UPDATE MODEL PARAMETERS
        
    
        _, predicted = torch.max(logits.data, 1)
        total_train += targets.size(0)
        correct_train += (predicted == targets).sum().item()
        ### LOGGING
        if not batch_idx % 250:
            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' 
#                   %(epoch+1, num_epochs, batch_idx, 
                    len(train_loader), cost))
            
    train_losses.append(r) 
    train_acc.append(100 * correct_train / total_train)
    #val_acc.append(test(model, test_loader))
    test_loss = 0   
    correct = 0
    total = 0 
    with torch.no_grad():
        for data, target in test_loader:
            output = student_net(data.to(device))
            loss = torch.nn.functional.cross_entropy(output, target.to(device))
            test_loss += loss.item()
            _, predicted = torch.max(logits.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
        
    val_acc.append(100 * correct/ total)
    test_loss /= len(test_loader)
    test_losses.append(test_loss)  

    student_net= student_net.eval() # eval mode to prevent upd. batchnorm params during inference
    
    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (
              epoch+1, num_epochs,100 * correct_train / total_train))

import matplotlib.pyplot as plt
plt.plot(train_losses, label='training loss')
plt.plot(test_losses, label='testing loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
plt.plot(train_acc, label='training acc')
plt.plot(val_acc, label='val acc')
plt.legend()
plt.show()



correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        inputs, labels = data
        inputs=inputs.to(device)
        labels=labels.to(device)
        outputs = student_net(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
print("Accuracy: {:.2f}%".format(100 * correct / total))



"""# With EMA """

student_net=CNN().to(device)
teacher_net=cnn

learning_rate = 0.001
num_epochs =10

# Define the optimizer and loss function
optimizer = torch.optim.Adam(student_net.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

# Set the temperature and alpha parameters for soft target generation
temperature = 4
alpha =0.5

train_losses=[]
test_losses = []
train_acc = []
val_acc = []

# Train the student network
for epoch in range(num_epochs):
    running_loss = 0.0
    
    # Set the networks to train mode
    student_net.train()
    teacher_net.train()
    correct_train = 0
    total_train = 0
    
    # Iterate over the training data
    for inputs, labels in train_loader:

        inputs, labels = inputs.to(device), labels.to(device)
        
        
        # Generate the soft and hard targets from the teacher network
        with torch.no_grad():
            soft_target = torch.nn.functional.softmax(teacher_net(inputs) / temperature, dim=1)
            hard_target = torch.zeros_like(soft_target)
            hard_target.scatter_(1, labels.unsqueeze(1), 1)
            target = alpha * soft_target + (1 - alpha) * hard_target
        
        # Forward pass through the student network
        outputs = student_net(inputs)
        loss = criterion(outputs, target)
        
        # Backward pass and optimization step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        ema_decay = 0.999
        with torch.no_grad():
            for student_param, teacher_param in zip(student_net.parameters(), teacher_net.parameters()):
                teacher_param.data.mul_(ema_decay).add_(1 - ema_decay, student_param.data)
        
        running_loss += loss.item()

        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()
        
            
    train_losses.append(running_loss / len(train_loader)) 
    train_acc.append(100 * correct_train / total_train)
    #val_acc.append(test(model, test_loader))
    test_loss = 0   
    correct = 0
    total = 0 
    with torch.no_grad():
        for data, target in test_loader:
            output = student_net(data.to(device))
            loss = torch.nn.functional.cross_entropy(output, target.to(device))
            test_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
    val_acc.append(100 * correct/ total)
    test_loss /= len(test_loader)
    test_losses.append(test_loss) 

  
    print(f"Epoch {epoch+1}/{num_epochs} - Training Loss: {running_loss/len(train_loader) :.4f}")

import matplotlib.pyplot as plt
plt.plot(train_losses, label='training loss')
plt.plot(test_losses, label='testing loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
plt.plot(train_acc, label='training acc')
plt.plot(val_acc, label='val acc')
plt.legend()
plt.show()

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        inputs, labels = data
        inputs=inputs.to(device)
        labels=labels.to(device)
        outputs = teacher_net(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
print("Accuracy: {:.2f}%".format(100 * correct / total))

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        inputs, labels = data
        inputs=inputs.to(device)
        labels=labels.to(device)
        outputs = student_net(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
print("Accuracy: {:.2f}%".format(100 * correct / total))

r =student_CNN().to(device)

teacher_net